{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cf0b2b",
   "metadata": {},
   "source": [
    "# MMathCoT-1M | Local Downloader, Validator, Packager\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b446f4",
   "metadata": {},
   "source": [
    "This notebook downloads all image sources required by `URSA-MATH/MMathCoT-1M`, validates coverage against the dataset's `image_url` column, builds a manifest, and packs **tar shards** where each member's **arcname equals `image_url`**. It also includes an optional step to upload to a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74861a1b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "---\n",
    "1. [Install Dependencies](#1-install-dependencies)\n",
    "2. [Configuration](#2-configuration)\n",
    "3. [Load Dataset Metadata](#3-load-dataset-metadata)\n",
    "4. [Download and Extract Sources](#4-download-and-extract-sources)\n",
    "5. [Per-Source Validation](#5-per-source-validation)\n",
    "6. [Global Manifest (URL → abs\\_path)](#6-global-manifest-url--abs_path)\n",
    "7. [Sharding: Build URL-Aligned Tar Files](#7-sharding-build-url-aligned-tar-files)\n",
    "8. [Quick Verification](#8-quick-verification)\n",
    "9. [Optional: Upload to Hugging Face](#9-optional-upload-to-hugging-face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b91602",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e055e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip -q install ipywidgets python-dotenv huggingface_hub datasets py7zr tqdm pandas pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ebf6e8",
   "metadata": {},
   "source": [
    "### 2. Configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f9d6f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] BASE_DIR   = /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images\n",
      "[CONFIG] HF_CACHE   = /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/_hf_cache\n",
      "[CONFIG] TEMP_DIR   = /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/_temp\n",
      "[CONFIG] SHARDS_DIR = /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(usecwd=True), override=False)\n",
    "\n",
    "# Root directory for downloads and outputs\n",
    "BASE_DIR = Path(\"/Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images\")\n",
    "HF_CACHE = BASE_DIR / \"_hf_cache\"\n",
    "TEMP_DIR = BASE_DIR / \"_temp\"\n",
    "SHARDS_DIR = BASE_DIR / \"shards\"\n",
    "\n",
    "# Tar shard size\n",
    "MAX_BYTES_PER_SHARD = 5_000_000_000 # (approx 5 GB)\n",
    "\n",
    "# Hugging Face dataset upload settings\n",
    "HF_DATASET_REPO_ID = \"michaelburiek/mmathcot1m-images\"\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "UPLOAD_TO_HF = False  # set True only when you're ready to upload\n",
    "\n",
    "# Create directories\n",
    "for d in [BASE_DIR, HF_CACHE, TEMP_DIR, SHARDS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[CONFIG] BASE_DIR   = {BASE_DIR}\")\n",
    "print(f\"[CONFIG] HF_CACHE   = {HF_CACHE}\")\n",
    "print(f\"[CONFIG] TEMP_DIR   = {TEMP_DIR}\")\n",
    "print(f\"[CONFIG] SHARDS_DIR = {SHARDS_DIR}\")\n",
    "\n",
    "if UPLOAD_TO_HF and not HF_TOKEN:\n",
    "    raise RuntimeError(\"UPLOAD_TO_HF=True but HF_TOKEN is not set in .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db231415",
   "metadata": {},
   "source": [
    "### 3. Load Dataset Metadata\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7193b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MMathCoT-1M dataset metadata...\n",
      "[SUCCESS] Loaded 1,019,059 entries\n",
      "[INFO] Unique image URLs: 574,534\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"[INFO] Loading MMathCoT-1M dataset metadata...\")\n",
    "dataset = load_dataset(\"URSA-MATH/MMathCoT-1M\", split=\"train\")\n",
    "print(f\"[SUCCESS] Loaded {len(dataset):,} entries\")\n",
    "\n",
    "unique_urls = []\n",
    "for i in range(len(dataset)):\n",
    "    u = dataset[i].get(\"image_url\", \"\")\n",
    "    if u:\n",
    "        unique_urls.append(u)\n",
    "unique_urls = sorted(set(unique_urls))\n",
    "print(f\"[INFO] Unique image URLs: {len(unique_urls):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2bbb4",
   "metadata": {},
   "source": [
    "### 4. Download and Extract Sources\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1bdd76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET 1/6: Geo170K\n",
      "================================================================================\n",
      "[SUCCESS] Extracted ~12,031 images\n",
      "\n",
      "================================================================================\n",
      "DATASET 2/6: MathV-360k\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data_images.zip: 100%|██████████| 38785/38785 [00:10<00:00, 3667.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Extracted ~38,734 images\n",
      "\n",
      "================================================================================\n",
      "DATASET 3/6: VarsityTutors\n",
      "================================================================================\n",
      "[SUCCESS] Extracted ~80,639 images\n",
      "\n",
      "================================================================================\n",
      "DATASET 4/6: DataEngine_Geometry\n",
      "================================================================================\n",
      "[SUCCESS] Extracted ~193,914 images\n",
      "\n",
      "================================================================================\n",
      "DATASET 5/6: Multimath\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images.zip: 100%|██████████| 353077/353077 [00:48<00:00, 7240.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Extracted ~353,076 images\n",
      "\n",
      "================================================================================\n",
      "DATASET 6/6: Mavis_Extra (partial in HF)\n",
      "================================================================================\n",
      "  \u001b[2m2025-10-13T01:13:22.635389Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://transfer.xethub.hf.co/xorbs/default/e0debc5fa67f4ba6cb19b04fb39ff12790228b0246b45cfa7b9702ca5f44fe54?X-Xet-Signed-Range=bytes%3D0-67095396&X-Xet-Session-Id=01K7DJ23HG7RR5ABQ5YA4FY5QC&Expires=1760321602&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly90cmFuc2Zlci54ZXRodWIuaGYuY28veG9yYnMvZGVmYXVsdC9lMGRlYmM1ZmE2N2Y0YmE2Y2IxOWIwNGZiMzlmZjEyNzkwMjI4YjAyNDZiNDVjZmE3Yjk3MDJjYTVmNDRmZTU0P1gtWGV0LVNpZ25lZC1SYW5nZT1ieXRlcyUzRDAtNjcwOTUzOTYmWC1YZXQtU2Vzc2lvbi1JZD0wMUs3REoyM0hHN1JSNUFCUTVZQTRGWTVRQyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MDMyMTYwMn19fV19&Signature=mQXzUFfGAOxqlCQNKIgjf0gBKfTRp2a8p2WpGV8VKXlj3D7CmykVEp9QZ2DKo2N7Fl9kmnBfpNAAaiiOp8gRoOx6Pra8S1kECENsQsjmOWi65H2DtZbshIBy2EEaAJbATiaeUIy4VlvNrAeKPr1k1IBirtVNvBtEbZlVm79rITLHSzBfWJkck19EVlzTl3VS7Vo6IoyvthTs7HA01cBydeoIPbCUNyKXfE-xDMtG52MtZYlOjbbs3xEGkViue27Rggu3UUV4kUPqhPZGXKPJ4~13rDQwsTmHi8A3u6iSk8xqRaH8GxLKCYCedGahYZCtRU~HrKYRuMIZr3IZkcFt9Q__&Key-Pair-Id=K2L8F4GPSG1IFC\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: nodename nor servname provided, or not known\" })) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:233\n",
      "\n",
      "  \u001b[2m2025-10-13T01:13:22.635442Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #0. Sleeping 398.183071ms before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "[SUCCESS] Extracted ~24,055 images\n",
      "\n",
      "[NOTE] The Hugging Face version of Mavis_Extra is intentionally partial (primarily 'function_wo').\n",
      "[NOTE] For complete MAVIS, use the original Google Drive source and place under BASE_DIR/Mavis_Extra.\n"
     ]
    }
   ],
   "source": [
    "import shutil, zipfile, py7zr\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "def count_images(path: Path):\n",
    "    exts = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.tif', '.tiff'}\n",
    "    return sum(1 for p in path.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts)\n",
    "\n",
    "def extract_7z(archive_path: Path, extract_dir: Path):\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with py7zr.SevenZipFile(archive_path, mode='r') as z:\n",
    "        z.extractall(path=extract_dir)\n",
    "\n",
    "def extract_zip(zip_path: Path, dest_dir: Path):\n",
    "    temp = dest_dir.parent / (dest_dir.name + \"_tmp\")\n",
    "    if temp.exists():\n",
    "        shutil.rmtree(temp)\n",
    "    temp.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        names = z.namelist()\n",
    "        for name in tqdm(names, desc=f\"Extracting {zip_path.name}\"):\n",
    "            z.extract(name, temp)\n",
    "    if dest_dir.exists():\n",
    "        shutil.rmtree(dest_dir)\n",
    "    shutil.move(str(temp), str(dest_dir))\n",
    "\n",
    "# Geo170K (URSA-MATH/MMathCoT-1M: Geo170K.7z)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET 1/6: Geo170K\")\n",
    "print(\"=\"*80)\n",
    "g_dir = BASE_DIR / \"Geo170K\"\n",
    "g_marker = g_dir / \".download_complete\"\n",
    "if not g_marker.exists():\n",
    "    arch = hf_hub_download(\"URSA-MATH/MMathCoT-1M\", filename=\"Geo170K.7z\", repo_type=\"dataset\", cache_dir=str(HF_CACHE))\n",
    "    extract_7z(Path(arch), g_dir)\n",
    "    print(f\"[SUCCESS] Extracted ~{count_images(g_dir):,} images\")\n",
    "    g_marker.touch()\n",
    "else:\n",
    "    print(\"[SKIP] Geo170K already present.\")\n",
    "\n",
    "# MathV-360k (Zhiqiang007/MathV360K: data_images.zip)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET 2/6: MathV-360k\")\n",
    "print(\"=\"*80)\n",
    "m_dir = BASE_DIR / \"MathV-360k\"\n",
    "m_marker = m_dir / \".download_complete\"\n",
    "if not m_marker.exists():\n",
    "    zpath = hf_hub_download(\"Zhiqiang007/MathV360K\", filename=\"data_images.zip\", repo_type=\"dataset\", cache_dir=str(HF_CACHE))\n",
    "    extract_zip(Path(zpath), m_dir)\n",
    "    print(f\"[SUCCESS] Extracted ~{count_images(m_dir):,} images\")\n",
    "    m_marker.touch()\n",
    "else:\n",
    "    print(\"[SKIP] MathV-360k already present.\")\n",
    "\n",
    "# VarsityTutors (URSA-MATH/MMathCoT-1M: VarsityTutors.7z)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET 3/6: VarsityTutors\")\n",
    "print(\"=\"*80)\n",
    "v_dir = BASE_DIR / \"VarsityTutors\"\n",
    "v_marker = v_dir / \".download_complete\"\n",
    "if not v_marker.exists():\n",
    "    arch = hf_hub_download(\"URSA-MATH/MMathCoT-1M\", filename=\"VarsityTutors.7z\", repo_type=\"dataset\", cache_dir=str(HF_CACHE))\n",
    "    extract_7z(Path(arch), v_dir)\n",
    "    print(f\"[SUCCESS] Extracted ~{count_images(v_dir):,} images\")\n",
    "    v_marker.touch()\n",
    "else:\n",
    "    print(\"[SKIP] VarsityTutors already present.\")\n",
    "\n",
    "# DataEngine_Geometry (URSA-MATH/MMathCoT-1M: DataEngine_Geometry.7z)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET 4/6: DataEngine_Geometry\")\n",
    "print(\"=\"*80)\n",
    "d_dir = BASE_DIR / \"DataEngine_Geometry\"\n",
    "d_marker = d_dir / \".download_complete\"\n",
    "if not d_marker.exists():\n",
    "    arch = hf_hub_download(\"URSA-MATH/MMathCoT-1M\", filename=\"DataEngine_Geometry.7z\", repo_type=\"dataset\", cache_dir=str(HF_CACHE))\n",
    "    extract_7z(Path(arch), d_dir)\n",
    "    print(f\"[SUCCESS] Extracted ~{count_images(d_dir):,} images\")\n",
    "    d_marker.touch()\n",
    "else:\n",
    "    print(\"[SKIP] DataEngine_Geometry already present.\")\n",
    "\n",
    "# Multimath (pengshuai-rin/multimath-300k: images.zip)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET 5/6: Multimath\")\n",
    "print(\"=\"*80)\n",
    "mm_dir = BASE_DIR / \"Multimath\"\n",
    "mm_marker = mm_dir / \".download_complete\"\n",
    "if not mm_marker.exists():\n",
    "    zpath = hf_hub_download(\"pengshuai-rin/multimath-300k\", filename=\"images.zip\", repo_type=\"dataset\", cache_dir=str(HF_CACHE))\n",
    "    extract_zip(Path(zpath), mm_dir)\n",
    "    print(f\"[SUCCESS] Extracted ~{count_images(mm_dir):,} images\")\n",
    "    mm_marker.touch()\n",
    "else:\n",
    "    print(\"[SKIP] Multimath already present.\")\n",
    "\n",
    "# Mavis_Extra (URSA-MATH/MMathCoT-1M: Mavis_Extra.7z)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET 6/6: Mavis_Extra (partial in HF)\")\n",
    "print(\"=\"*80)\n",
    "me_dir = BASE_DIR / \"Mavis_Extra\"\n",
    "me_marker = me_dir / \".download_complete\"\n",
    "if not me_marker.exists():\n",
    "    arch = hf_hub_download(\"URSA-MATH/MMathCoT-1M\", filename=\"Mavis_Extra.7z\", repo_type=\"dataset\", cache_dir=str(HF_CACHE))\n",
    "    extract_7z(Path(arch), me_dir)\n",
    "    print(f\"[SUCCESS] Extracted ~{count_images(me_dir):,} images\")\n",
    "    me_marker.touch()\n",
    "else:\n",
    "    print(\"[SKIP] Mavis_Extra already present.\")\n",
    "\n",
    "print(\"\\n[NOTE] The Hugging Face version of Mavis_Extra is intentionally partial (primarily 'function_wo').\")\n",
    "print(\"[NOTE] For complete MAVIS, use the original Google Drive source and place under BASE_DIR/Mavis_Extra.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bfe1d",
   "metadata": {},
   "source": [
    "### 5. Per-Source Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42cb062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo170K                expected=52,196 found=52,196 rate=100.00%\n",
      "MathV-360k             expected=319,619 found=319,619 rate=100.00%\n",
      "VarsityTutors          expected=55,162 found=55,162 rate=100.00%\n",
      "DataEngine_Geometry    expected=186,471 found=186,471 rate=100.00%\n",
      "Multimath              expected=264,205 found=264,205 rate=100.00%\n",
      "Mavis_Extra            expected=141,406 found=141,406 rate=100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL          expected=1,019,059 found=1,019,059 rate=100.00%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse, unquote\n",
    "\n",
    "def validate_source(source_name: str, source_root: Path, ds) -> dict:\n",
    "    exts = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.tif', '.tiff'}\n",
    "    # Build filename index for this source\n",
    "    files = [p for p in source_root.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
    "    index = defaultdict(list)\n",
    "    for p in files:\n",
    "        index[p.name].append(p)\n",
    "\n",
    "    # Count expected from dataset\n",
    "    expected = 0\n",
    "    found = 0\n",
    "    missing = []\n",
    "\n",
    "    for i in range(len(ds)):\n",
    "        u = ds[i].get(\"image_url\",\"\")\n",
    "        if not u.startswith(source_name + \"/\"):\n",
    "            continue\n",
    "        expected += 1\n",
    "        fname = Path(unquote(urlparse(u).path)).name\n",
    "        if fname in index:\n",
    "            found += 1\n",
    "        else:\n",
    "            missing.append(u)\n",
    "\n",
    "    rate = (found / expected * 100) if expected else 0.0\n",
    "    return {\"source\": source_name, \"expected\": expected, \"found\": found, \"rate\": rate, \"missing\": missing}\n",
    "\n",
    "sources = {\n",
    "    \"Geo170K\": (BASE_DIR / \"Geo170K\" / \"Geo170K\"),\n",
    "    \"MathV-360k\": (BASE_DIR / \"MathV-360k\" / \"data_images\"),\n",
    "    \"VarsityTutors\": (BASE_DIR / \"VarsityTutors\" / \"VarsityTutors\"),\n",
    "    \"DataEngine_Geometry\": (BASE_DIR / \"DataEngine_Geometry\" / \"DataEngine_Geometry\"),\n",
    "    \"Multimath\": (BASE_DIR / \"Multimath\" / \"RGB_images\"),\n",
    "    \"Mavis_Extra\": (BASE_DIR / \"Mavis_Extra\" / \"Mavis_Extra\")\n",
    "}\n",
    "\n",
    "summary = []\n",
    "for name, root in sources.items():\n",
    "    if root.exists():\n",
    "        res = validate_source(name, root, dataset)\n",
    "        summary.append(res)\n",
    "        print(f\"{name:<22} expected={res['expected']:,} found={res['found']:,} rate={res['rate']:.2f}%\")\n",
    "        if res[\"missing\"]:\n",
    "            miss_path = BASE_DIR / f\"{name}_missing.txt\"\n",
    "            miss_path.write_text(\"\\n\".join(res[\"missing\"]) + \"\\n\")\n",
    "            print(f\"  -> wrote missing list: {miss_path}\")\n",
    "    else:\n",
    "        print(f\"{name:<22} [WARN] root not found: {root}\")\n",
    "\n",
    "overall_expected = sum(s[\"expected\"] for s in summary)\n",
    "overall_found = sum(s[\"found\"] for s in summary)\n",
    "overall_rate = (overall_found / overall_expected * 100) if overall_expected else 0.0\n",
    "print(\"-\"*80)\n",
    "print(f\"OVERALL          expected={overall_expected:,} found={overall_found:,} rate={overall_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b58888",
   "metadata": {},
   "source": [
    "### 6. Global Manifest (URL → abs_path)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ef54fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Resolved:   574,534\n",
      "[INFO] Unresolved: 0\n",
      "[INFO] Wrote manifest: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/manifest.csv\n",
      "[INFO] Wrote manifest: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/manifest.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse: BASE_DIR, unique_urls already defined.\n",
    "\n",
    "# Point each source to the folder whose *relative path* matches the URL after \"<source_name>/\"\n",
    "# (i.e., no extra duplicated prefix in the key)\n",
    "sources_manifest = {\n",
    "    \"Geo170K\":             BASE_DIR / \"Geo170K\" / \"Geo170K\",\n",
    "    \"MathV-360k\":          BASE_DIR / \"MathV-360k\" / \"data_images\",\n",
    "    \"VarsityTutors\":       BASE_DIR / \"VarsityTutors\" / \"VarsityTutors\",\n",
    "    \"DataEngine_Geometry\": BASE_DIR / \"DataEngine_Geometry\" / \"DataEngine_Geometry\",\n",
    "    \"Multimath\":           BASE_DIR / \"Multimath\",                 # URLs include \"RGB_images/...\"\n",
    "    \"Mavis_Extra\":         BASE_DIR / \"Mavis_Extra\" / \"Mavis_Extra\",\n",
    "}\n",
    "\n",
    "EXTS = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.tif', '.tiff'}\n",
    "\n",
    "def build_url_to_abs_index(sources_map: dict[str, Path]) -> dict[str, Path]:\n",
    "    idx: dict[str, Path] = {}\n",
    "    for source_name, root in sources_map.items():\n",
    "        root = Path(root)\n",
    "        if not root.exists():\n",
    "            print(f\"[WARN] Source root missing: {source_name} -> {root}\")\n",
    "            continue\n",
    "        for p in root.rglob(\"*\"):\n",
    "            if p.is_file() and p.suffix.lower() in EXTS:\n",
    "                rel_under_root = p.relative_to(root).as_posix()\n",
    "                url_key = f\"{source_name}/{rel_under_root}\"\n",
    "                # keep first occurrence deterministically\n",
    "                if url_key not in idx:\n",
    "                    idx[url_key] = p\n",
    "    return idx\n",
    "\n",
    "# Build mapping and resolve\n",
    "url_to_abs = build_url_to_abs_index(sources_manifest)\n",
    "\n",
    "rows, unresolved = [], []\n",
    "for u in unique_urls:\n",
    "    p = url_to_abs.get(u)\n",
    "    if p and p.exists() and p.suffix.lower() in EXTS:\n",
    "        rows.append({\n",
    "            \"image_url\": u,\n",
    "            \"rel_path\": u,        # arcname inside tar must equal image_url\n",
    "            \"abs_path\": str(p),\n",
    "            \"source\": u.split(\"/\", 1)[0],\n",
    "        })\n",
    "    else:\n",
    "        unresolved.append(u)\n",
    "\n",
    "print(f\"[INFO] Resolved:   {len(rows):,}\")\n",
    "print(f\"[INFO] Unresolved: {len(unresolved):,}\")\n",
    "\n",
    "if unresolved:\n",
    "    out = BASE_DIR / \"unresolved_image_urls.txt\"\n",
    "    out.write_text(\"\\n\".join(unresolved) + \"\\n\")\n",
    "    print(f\"[WARN] Wrote unresolved list to: {out}\")\n",
    "    print(\"First 10 unresolved:\")\n",
    "    for x in unresolved[:10]:\n",
    "        print(\"  -\", x)\n",
    "\n",
    "# Save manifest\n",
    "manifest_csv = BASE_DIR / \"manifest.csv\"\n",
    "manifest_parquet = BASE_DIR / \"manifest.parquet\"\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(manifest_csv, index=False)\n",
    "try:\n",
    "    df.to_parquet(manifest_parquet, index=False)\n",
    "    print(f\"[INFO] Wrote manifest: {manifest_csv}\")\n",
    "    print(f\"[INFO] Wrote manifest: {manifest_parquet}\")\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] Wrote manifest: {manifest_csv}\")\n",
    "    print(\"[WARN] Parquet write failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98b2ef",
   "metadata": {},
   "source": [
    "### 7. Sharding: Build URL-Aligned Tar Files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2cc01fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleted 0 old tar(s)\n",
      "[INFO] Creating 6 shard(s) with max_bytes=5,000,000,000\n",
      "[INFO] Wrote /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-000.tar (95538 files)\n",
      "[INFO] Wrote /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-001.tar (45123 files)\n",
      "[INFO] Wrote /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-002.tar (31963 files)\n",
      "[INFO] Wrote /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-003.tar (29877 files)\n",
      "[INFO] Wrote /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-004.tar (49454 files)\n",
      "[INFO] Wrote /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-005.tar (322579 files)\n",
      "[DONE] Files packed: 574534\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Clean out any old shards\n",
    "deleted = 0\n",
    "for p in SHARDS_DIR.glob(\"*.tar\"):\n",
    "    try:\n",
    "        p.unlink()\n",
    "        deleted += 1\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not delete\", p, e)\n",
    "print(f\"[INFO] Deleted {deleted} old tar(s)\")\n",
    "\n",
    "# 2) Size-based sharding\n",
    "def shard_by_bytes(rows, max_bytes: int):\n",
    "    shards, current, used = [], [], 0\n",
    "    for r in rows:\n",
    "        try:\n",
    "            sz = Path(r[\"abs_path\"]).stat().st_size\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        if current and (used + sz > max_bytes):\n",
    "            shards.append(current)\n",
    "            current, used = [], 0\n",
    "        current.append(r)\n",
    "        used += sz\n",
    "    if current:\n",
    "        shards.append(current)\n",
    "    return shards\n",
    "\n",
    "# Sort for deterministic output (use rel_path which is the URL)\n",
    "rows_sorted = sorted(rows, key=lambda r: r[\"rel_path\"])\n",
    "shards = shard_by_bytes(rows_sorted, MAX_BYTES_PER_SHARD)\n",
    "print(f\"[INFO] Creating {len(shards)} shard(s) with max_bytes={MAX_BYTES_PER_SHARD:,}\")\n",
    "\n",
    "# Normalize tar member metadata for reproducibility\n",
    "def _normalize_tarinfo(ti: tarfile.TarInfo) -> tarfile.TarInfo:\n",
    "    ti.uid = 0\n",
    "    ti.gid = 0\n",
    "    ti.uname = \"root\"\n",
    "    ti.gname = \"root\"\n",
    "    ti.mtime = 0\n",
    "    ti.mode = ti.mode & 0o777\n",
    "    return ti\n",
    "\n",
    "# 3) Write shards\n",
    "total = 0\n",
    "for i, group in enumerate(shards):\n",
    "    tar_path = SHARDS_DIR / f\"images-{i:03d}.tar\"\n",
    "    with tarfile.open(tar_path, \"w\", format=tarfile.PAX_FORMAT) as tar:\n",
    "        for r in group:\n",
    "            src = Path(r[\"abs_path\"])\n",
    "            arc = r[\"rel_path\"]  # must equal image_url\n",
    "            tar.add(src, arcname=arc, filter=_normalize_tarinfo)\n",
    "    print(f\"[INFO] Wrote {tar_path} ({len(group)} files)\")\n",
    "    total += len(group)\n",
    "\n",
    "print(f\"[DONE] Files packed: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ef33a",
   "metadata": {},
   "source": [
    "### 8. Quick Verification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0690f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified OK: 25  FAIL: 0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "sample = random.sample(rows, min(25, len(rows)))\n",
    "ok, bad = 0, 0\n",
    "for r in sample:\n",
    "    try:\n",
    "        with Image.open(r[\"abs_path\"]) as im:\n",
    "            im.verify()\n",
    "        with Image.open(r[\"abs_path\"]) as im:\n",
    "            _ = im.size\n",
    "        ok += 1\n",
    "    except Exception as e:\n",
    "        bad += 1\n",
    "        print(\"[FAIL]\", r[\"abs_path\"], e)\n",
    "\n",
    "print(f\"Verified OK: {ok}  FAIL: {bad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9265d3d",
   "metadata": {},
   "source": [
    "### 9. Optional: Upload to Hugging Face\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3729d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(usecwd=True), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a259c20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6893c075add77868b890bf13', 'name': 'michaelburiek', 'fullname': 'Michael Buriek', 'isPro': False, 'avatarUrl': '/avatars/63cb09a7595ab144ff1ed44d56e700ac.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'dcvlr', 'role': 'fineGrained', 'createdAt': '2025-10-13T01:56:32.122Z', 'fineGrained': {'canReadGatedRepos': False, 'global': [], 'scoped': [{'entity': {'_id': '6893c075add77868b890bf13', 'type': 'user', 'name': 'michaelburiek'}, 'permissions': ['repo.content.read', 'repo.write']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi()\n",
    "token = os.getenv(\"HF_TOKEN\", \"\")\n",
    "print(api.whoami(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ecbc6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Authenticated as: michaelburiek\n",
      "[INFO] Ensuring dataset repo exists: michaelburiek/mmathcot1m-images\n",
      "[INFO] Uploading 9 items (~28.12 GB) to hf://datasets/michaelburiek/mmathcot1m-images\n",
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/manifest.csv  →  manifest.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████|  151MB /  151MB, 7.51MB/s  \n",
      "New Data Upload: 100%|██████████|  151MB /  151MB, 7.51MB/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/manifest.parquet  →  manifest.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 33.5MB / 33.5MB, 1.61MB/s  \n",
      "New Data Upload: 100%|██████████| 23.3MB / 23.3MB, 1.12MB/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-000.tar  →  shards/images-000.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 1): 100%|█████████▉| 5.07GB / 5.07GB,  0.00B/s  \n",
      "New Data Upload: 100%|██████████| 4.97GB / 4.97GB,  0.00B/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-001.tar  →  shards/images-001.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 5.03GB / 5.03GB,  0.00B/s  \n",
      "New Data Upload: 100%|██████████| 4.82GB / 4.82GB,  0.00B/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-002.tar  →  shards/images-002.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 1): 100%|█████████▉| 5.02GB / 5.02GB,  0.00B/s  \n",
      "New Data Upload: 100%|██████████| 4.69GB / 4.69GB,  0.00B/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-003.tar  →  shards/images-003.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 5.02GB / 5.02GB,  0.00B/s  \n",
      "New Data Upload: 100%|██████████| 3.17GB / 3.17GB,  0.00B/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-004.tar  →  shards/images-004.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 1): 100%|█████████▉| 5.04GB / 5.04GB,  0.00B/s  \n",
      "New Data Upload: 100%|██████████| 3.48GB / 3.48GB,  0.00B/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/shards/images-005.tar  →  shards/images-005.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 1):  93%|█████████▎| 4.48GB / 4.81GB, 2.19MB/s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-13T04:13:17.435280Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 502. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 1): 100%|█████████▉| 4.81GB / 4.81GB,  0.00B/s  \n",
      "New Data Upload: 100%|██████████| 4.56GB / 4.56GB,  0.00B/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading: /Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images/_README.md  →  README.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelburiek/Documents/GitHub/dcvlr/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:9717: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Uploaded 9/9 items to michaelburiek/mmathcot1m-images\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "from pathlib import Path\n",
    "import os, time\n",
    "\n",
    "# ---------- Config ----------\n",
    "BASE_DIR   = Path(\"/Users/michaelburiek/Documents/GitHub/dcvlr/mmath_cot_1m/images\")\n",
    "SHARDS_DIR = BASE_DIR / \"shards\"\n",
    "\n",
    "# Prefer the value from your earlier config if set; otherwise default to your account/repo\n",
    "REPO_ID = os.environ.get(\"HF_DATASET_REPO_ID\", \"\").strip() or \"michaelburiek/mmathcot1m-images\"\n",
    "TOKEN   = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
    "\n",
    "if not TOKEN:\n",
    "    raise RuntimeError(\"HF_TOKEN is empty. Put it in your .env or export it in your shell, then re-run.\")\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# ---------- Sanity checks ----------\n",
    "who = api.whoami(token=TOKEN)\n",
    "acct = who.get(\"name\") or (who.get(\"orgs\") or [{}])[0].get(\"name\")\n",
    "print(f\"[INFO] Authenticated as: {acct}\")\n",
    "\n",
    "wanted_ns = REPO_ID.split(\"/\", 1)[0]\n",
    "if wanted_ns != acct:\n",
    "    raise RuntimeError(\n",
    "        f\"Namespace mismatch: REPO_ID starts with '{wanted_ns}' but your token is for '{acct}'. \"\n",
    "        f\"Fix REPO_ID to '{acct}/mmathcot1m-images' or use a token that belongs to '{wanted_ns}'.\"\n",
    "    )\n",
    "\n",
    "# ---------- Ensure dataset repo exists (idempotent) ----------\n",
    "print(f\"[INFO] Ensuring dataset repo exists: {REPO_ID}\")\n",
    "create_repo(repo_id=REPO_ID, repo_type=\"dataset\", exist_ok=True, token=TOKEN)\n",
    "\n",
    "# ---------- Collect files to upload (no shard changes) ----------\n",
    "to_upload = []\n",
    "\n",
    "m_csv  = BASE_DIR / \"manifest.csv\"\n",
    "m_parq = BASE_DIR / \"manifest.parquet\"\n",
    "if m_csv.exists():  to_upload.append((m_csv,  \"manifest.csv\"))\n",
    "if m_parq.exists(): to_upload.append((m_parq, \"manifest.parquet\"))\n",
    "\n",
    "unresolved = BASE_DIR / \"unresolved_image_urls.txt\"\n",
    "if unresolved.exists() and unresolved.stat().st_size > 0:\n",
    "    to_upload.append((unresolved, \"unresolved_image_urls.txt\"))\n",
    "\n",
    "shard_files = sorted(SHARDS_DIR.glob(\"images-*.tar\"))\n",
    "if not shard_files:\n",
    "    raise RuntimeError(f\"No shard files found in {SHARDS_DIR}\")\n",
    "for sf in shard_files:\n",
    "    to_upload.append((sf, f\"shards/{sf.name}\"))\n",
    "\n",
    "# Lightweight README\n",
    "readme_text = \"\"\"# MMathCoT-1M Images (Sharded)\n",
    "\n",
    "Local mirror of image assets referenced by the\n",
    "[`URSA-MATH/MMathCoT-1M`](https://huggingface.co/datasets/URSA-MATH/MMathCoT-1M) dataset.\n",
    "\n",
    "- Shards: tar files in `shards/`\n",
    "- Manifest: `manifest.csv` and `manifest.parquet`\n",
    "\n",
    "Each tar’s internal arcname equals the original `image_url` from the dataset.\n",
    "\"\"\"\n",
    "readme_tmp = BASE_DIR / \"_README.md\"\n",
    "readme_tmp.write_text(readme_text)\n",
    "to_upload.append((readme_tmp, \"README.md\"))\n",
    "\n",
    "# ---------- Uploader with simple retries ----------\n",
    "def upload_with_retries(path: Path, dest: str, max_retries: int = 4) -> bool:\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=str(path),\n",
    "                path_in_repo=dest,\n",
    "                repo_id=REPO_ID,\n",
    "                repo_type=\"dataset\",\n",
    "                token=TOKEN,\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                print(f\"[ERROR] Failed: {path} → {dest}: {e}\")\n",
    "                return False\n",
    "            backoff = min(60, 2 ** attempt)\n",
    "            print(f\"[WARN] Retry {attempt}/{max_retries} in {backoff}s for {dest}: {e}\")\n",
    "            time.sleep(backoff)\n",
    "\n",
    "# ---------- Size summary ----------\n",
    "def _fmt_bytes(n: int) -> str:\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]\n",
    "    i = 0; x = float(n)\n",
    "    while x >= 1024 and i < len(units)-1:\n",
    "        x /= 1024; i += 1\n",
    "    return f\"{x:.2f} {units[i]}\"\n",
    "\n",
    "total_bytes = sum(p.stat().st_size for p, _ in to_upload if p.exists())\n",
    "print(f\"[INFO] Uploading {len(to_upload)} items (~{_fmt_bytes(total_bytes)}) to hf://datasets/{REPO_ID}\")\n",
    "\n",
    "# ---------- Do uploads (idempotent; overwrites same paths only) ----------\n",
    "ok = 0\n",
    "for p, dest in to_upload:\n",
    "    print(f\"[INFO] Uploading: {p}  →  {dest}\")\n",
    "    if upload_with_retries(p, dest):\n",
    "        ok += 1\n",
    "\n",
    "print(f\"[DONE] Uploaded {ok}/{len(to_upload)} items to {REPO_ID}\")\n",
    "\n",
    "# Cleanup temp README file\n",
    "try:\n",
    "    readme_tmp.unlink(missing_ok=True)\n",
    "except Exception:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
